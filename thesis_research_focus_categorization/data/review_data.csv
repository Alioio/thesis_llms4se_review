reference,Title,year,validate_results,contribute_to_academia,task_objective_group,task,se_task,se_activity,ml_task,research_focus,Datasets,Baselines,Metrics,Validation Methods,Ground Truth ,Reliablity_Summary,Relevance_Summary,Reliablity_Score,Relevance_Score,ground_truth_score,baseline_score,metrics_score,validation_score,datasets_score,ground_truth_note,baseline_note,metrics_note,validation_note,datasets_note,baseline
alhamed2022evaluation,evaluation of context-aware language models and experts for effort estimation of software maintenance issues,2022,3.0,3.0,1,Effort and Resource Estimation,Effort estimation,software_management,classification,existing_solution_evaluation,"DEEP-SE, JOSSE, Porru, PPI, Industrial Dataset","DT with TF-IDF, DT with learned embeddings, Human Expert Estimations","AUC-ROC, F-score",Cross-validation,Actual time spent/ SP ,"The evaluation strategy employs multiple datasets and robust cross-validation, ensuring consistency and reproducibility. Metrics such as AUC-ROC and F-score are well-suited to effort estimation tasks, as they address class imbalance effectively. Framing effort estimation as a categorization problem (e.g., Planning Poker-inspired categorization) enhances the reliability of the evaluation by balancing calibration and informativeness in the context of imbalanced data.","The use of actual effort data (actual time and story points) as ground truth strengthens the relevance of the evaluation to real-world applications. Furthermore, categorizing effort enables the application of evaluation metrics (AUC-ROC and F-score) that offer more meaningful and actionable insights. This approach aligns well with practical needs in early project phases, where rapid and informed estimations are critical for decision-making.",4,4,1,1,1,1,1,High-quality ground truth using actual time spent,Multiple baseline types including ML and human experts,"Comprehensive metrics (AUC-ROC, F-score) addressing class imbalance",Robust cross-validation strategy,Multiple diverse datasets with real-world data,nan
fu2022gpt2sp,GPT2SP: A Transformer-Based Agile Story Point Estimation Approach,2023,3.0,3.0,1,Effort and Resource Estimation,Effort Estimation,software_management,regression,direct_solution,16 open-source projects,"Traditional ML, deep learning, analogy-based methods",MAE,Within- and cross-project scenarios,Estimated SP ,"Robust statistical methods ensure meaningful comparisons across models. However, the lack of transparency in MAE interpretation and uniformity in story point ranges complicates the interpretation of evaluation results, potentially limiting their reproducibility and clarity.","By covering both within- and cross-project scenarios, the evaluation provides valuable insights into the model's ability to generalize from other projects to new projects with limited information, addressing a critical aspect of real-world applicability. However, a significant limitation lies in the use of estimated story points as ground truth, as these estimates are inherently uncertain, reducing the evaluation’s ability to reliably assess real-world performance. While a user study highlights the perceived utility of AI-assisted predictions, reliance on MAE alone does not fully capture the solution’s capability to provide ""fair-enough"" estimations. Additionally, evaluation on open-source projects limits insights to the generalization capability to industrial settings.",3,3,-1,1,0,1,1,Ground truth based on uncertain story point estimates,Comprehensive baseline comparison across paradigms,Single metric (MAE) limits insight,Within and cross-project validation,Multiple open-source projects with good diversity,nan
li2024fine,Fine-SE: Integrating Semantic Features and Expert Features for Software Effort Estimation,2024,,,1,Effort and Resource Estimation,Effort Estimation,software_management,regression,direct_solution,"JIRA projects, 17 enterprise projects","TF-IDF, LDA, Deep-SE, GPT2SP","MAE, MMRE, PRED(50)",Cross-validation,Actual effort in person-months ,"The evaluation employs cross-validation and transparently presents metric scores (MAE, MMRE, PRED(50)) for each project in comprehensible tables, enhancing reliability and clarity. Metrics assess calibration and informativeness, providing meaningful insights into the evaluation's robustness and effectiveness.","The evaluation combines open-source and industrial projects, providing a diverse set of real-world scenarios for assessing generalizability. Additionally, the inclusion of within-project and cross-project evaluations offers complementary perspectives on model performance. The use of actual effort in person-months as ground truth is a major strength, ensuring the evaluation reflects real-world applicability. Metrics like PRED(50) highlight calibration and informativeness, further addressing practical utility.",4,4,1,1,1,1,1,Actual effort in person-months as ground truth,Diverse baseline approaches,"Multiple complementary metrics (MAE, MMRE, PRED)",Cross-validation with clear methodology,Mixed dataset (open-source and industrial),nan
isotani2021duplicate,duplicate bug report detection by using sentence embedding and fine-tuning,2021,,,1,Bug Triage and Assignment,Duplicate Bug Report Detection,software_maintenance,generation,direct_solution,NTT Corporation bug reports,"TF-IDF, LDA",MAP,5-fold cross-validation,Human-labeled duplicates and non-duplicates ,"Small dataset size (51 bug reports) and high duplicate ratio (90\%) introduce bias, inflating performance metrics. Reliance on MAP alone limits comprehensive evaluation. Basic baselines reduce robustness.",Homogeneous dataset and unrepresentative duplicate frequency affect real-world applicability. Lack of user studies and scalability considerations limits practical utility.,2,2,-1,-1,-1,1,-1,Manual labeling without quality metrics,Limited baseline comparison,Single metric (MAP) without context,5-fold cross-validation implemented,Small dataset (51 reports) with bias,nan
lee2022light,a light bug triage framework for applying large pre-trained language model,2023,3.0,3.0,1,Bug Triage and Assignment,Bug triage,software_maintenance,classification,direct_solution,"Google Chromium, Mozilla Core, Mozilla Firefox, Private Industrial Dataset","Transformer-based methods, Traditional ML",acc@k (Top-k Accuracy),Real-world benchmarking,Actual developers who fixed each bug report ,"The evaluation employs cross-validation and a large number of bug reports, enhancing reliability through robust statistical validation and dataset size. While Top-k Accuracy (acc@k) reflects practical use cases, the lack of complementary metrics limits a more comprehensive understanding of the evaluation results. The absence of qualitative evaluation and inference time analysis also constrains the assessment of broader aspects of evaluation robustness.","The use of real-world datasets ensures the evaluation aligns with practical bug triage scenarios. Metrics like acc@k address real-world needs for prioritizing the most likely relevant predictions. However, the evaluation is constrained by the focus on open-source datasets (limited to the browser development domain) and an unspecified industrial dataset domain, which affects generalizability to other contexts.",3,4,1,1,0,1,1,Actual developer assignments as ground truth,Multiple transformer and ML baselines,Limited to acc@k metrics,Real-world benchmarking approach,Large-scale multi-source dataset,nan
kou2023automated,Automated Summarization of Stack Overflow Posts,2023,3.0,3.0,1,Summarization and Knowledge Extraction,Code review and software/code classification,software_maintenance,generation,direct_solution,"SOSum dataset, additional labeled posts","Heuristic-based, BERT-based, unservised method","Precision, Recall, F1-score","10-fold cross-validation, user study",Human-labeled summaries ,"The evaluation employs cross-validation, quantitative metrics (precision, recall, F1-score), and a qualitative user study, enhancing evaluation robustness. Diverse datasets and baseline models further strengthen reliability. However, the small size of the user study limits generalizability.","The evaluation provides insights into performance across different post types (e.g., how-to, conceptual, bug-fixing), addressing practical use cases for developers. Measuring helpfulness through a user study offers valuable real-world feedback, but the evaluation could have been further strengthened by assessing time savings, a critical factor in developer productivity. Additionally, the lack of scalability metrics for larger threads limits broader applicability.",4,4,1,1,1,1,1,Human-labeled summaries with validation,Multiple paradigm baselines,Multiple complementary metrics,Cross-validation plus user study,SOSum dataset with additional labeled posts,nan
ciborowska2022fast,fast changeset-based bug localization with bert,2022,3.0,3.0,2,Bug Detection and Localization,Bug report related,software_maintenance,classification,direct_solution,"AspectJ, JDT, PDE, SWT, Tomcat, ZXing (OS)","Locus, TBERT-Single, TBERT-Siamese","Precision@K, MAP, MRR","Ablation (granularity, encoding), Runtime analysis",Manually validated mappings (SZZ error mitigated) ,"The evaluation employs a manually validated dataset consisting of six open-source software projects and applies well-established IR metrics. Comparative baselines are rigorously considered, and multiple encoding strategies (\textbf{D, ARC, ARCL}) and data granularities (\textbf{changesets, changeset-files, hunks}) are evaluated. However, the reliance on datasets limited to \textbf{open-source projects} restricts the generalizability of the findings to proprietary or industrial software systems.","The chosen evaluation metrics (\textbf{Precision@K, MAP, MRR}) align well with the task's objectives by effectively measuring retrieval accuracy and ranking quality. Additionally, the study lacks \textbf{cross-project validation}, which limits insights into the model's generalization to unseen projects. Runtime performance is thoroughly assessed, addressing the practical need for efficiency in real-world settings. However, the evaluation omits qualitative developer feedback, integration testing in CI/CD workflows, and validation of practical benefits such as time savings or bug-fixing efficiency. Moreover, the study does not report \textbf{training or fine-tuning times}, which are critical for assessing deployment feasibility in resource-constrained environments.",3,4,1,1,1,1,-1,Manually validated mappings,Multiple approach comparison,IR-specific metrics suite,Ablation and runtime analysis,Limited to open-source projects,nan
gomes2023bert,bert- and tf-idf-based feature extraction for long-lived bug prediction in floss: a comparative study,2023,2.0,3.0,2,Bug Detection and Localization,Bug prediction,software_maintenance,generation,existing_solution_evaluation,"Eclipse, Freedesktop, GCC, Gnome, Mozilla, WineHQ (OS, 50K+ reports)","TF-IDF, SVM, RF, k-NN, NB, NN",Balanced Accuracy,"10x5 CV, Wilcoxon test (95\%)",Bug fix times (1-year threshold) ,Six publicly available datasets ensure transparency and reproducibility. A consistent one-year labeling threshold and a 75/25 train-test split with cross-validation provide robustness. Balanced Accuracy effectively addresses class imbalance.,"The comparison between BERT and TF-IDF highlights practical trade-offs. Balanced Accuracy serves as the primary metric, but F1-Score could better address false positives and negatives. Insights are limited by a within-project evaluation, lack of training/fine-tuning time details, and absence of qualitative validation or inference time analysis limiting insights to practical deployment of the solution.",3,3,1,1,1,1,1,Clear temporal threshold for labeling,Multiple ML paradigm baselines,Balanced metrics for imbalance,Cross-validation with statistical tests,Large-scale multi-project dataset,nan
liu2024testing,Testing the limits: Unusual text inputs generation for mobile app crash detection with large language model,2023,,,2,Bug Detection and Localization,Mobile app crash detection,software_quality_assurance,classification,direct_solution,"36 widgets, 31 Android apps (OS)","18 baselines: fuzzing, mutation, string analysis, GUI tools","Detection rate, Attempts, Time","Ablation (modules), GUI integration, Manual validation",Crash reproducibility verified manually and by developers ,"The study systematically evaluates \textbf{InputBlaster} using key metrics and module-level ablation studies. Manual validation ensures crash reproducibility and correctness. The dataset was carefully constructed to prevent overlaps with training data, enhancing the robustness of the assessment. However, the evaluation is based on a relatively small set of \textbf{36 widgets across 31 apps}, which may limit generalizability. Additionally, the \textbf{distribution of input widget types (e.g., login fields, search boxes, numeric fields)} is not explicitly reported, raising questions about the dataset's representativeness.","The evaluation demonstrates alignment with real-world bug detection goals. Integration with automated GUI testing tools showcases practical applicability, with positive feedback from developers validating the findings. However, aspects such as deployment overheads remain underexplored, and the tool's ability to detect subtle usability or logical errors is not thoroughly investigated.",3,4,1,1,1,1,-1,Manual validation by developers,Comprehensive baseline suite,Multiple effectiveness metrics,Ablation and GUI integration,Limited widget sample size,nan
huang2024crashtranslator,CrashTranslator: Automatically Reproducing Mobile Application Crashes Directly from Stack Trace,2024,,,2,Bug Detection and Localization,Bug reproduction,software_maintenance,generation,direct_solution,"75 crash reports, 58 Android apps (OS, IND)","ReCDroid, Monkey, Humanoid, Ape, Q-Testing","Success rate, Time to reproduce","Ablation (scorer impact), Manual validation (14 testers)",Automated reproduction confirmed manually and systematically ,"The study systematically evaluates \textbf{CrashTranslator} using metrics directly aligned with crash reproduction goals, such as success rate and reproducing time. Validation combines automated scripts, manual verification, and ablation studies, ensuring robust assessment. However, the dataset size is relatively modest (\textbf{75 crash reports across 58 apps}), and the diversity of app types and crash scenarios remains underexplored. Additionally, manual validation relied on postgraduate testers, limiting real-world representativeness.","Metrics align with real-world crash reproduction goals, emphasizing efficiency and effectiveness. Integration with established tools and datasets demonstrates applicability. However, deployment overheads remain unassessed, and the tool's ability to handle complex user-specific or environmental preconditions is not thoroughly evaluated.",3,4,1,1,1,1,-1,Systematic reproduction validation,Multiple tool-based baselines,Task-specific success metrics,Ablation and manual validation,Limited crash report sample,nan
mastropaolo2022using,using deep learning to generate complete log statements,2022,3.0,3.0,2,Automated Logging,logging,software_maintenance,generation,direct_solution,"7,125 Java methods, 1,465 Log4j repos (OS)","Ablation: No Pre-training, Multi-Task, LogStmt-Task, Denoising-Task","Accuracy (level, location), BLEU-4","Manual validation (300 errors), Ablation studies",Assumed correctness of developer-written logs ,"The dataset avoids duplicate methods between training and test sets, but it is unclear whether repositories in the test set were entirely unseen during training, raising concerns about cross-repository generalizability. Manual validation focused on \textbf{300 incorrect predictions}, categorizing them into semantically equivalent, meaningful but different, and meaningless. The ground truth dataset, was not reviewed for noise or inconsistencies, leaving potential quality issues unaddressed.","Metrics address \textbf{log placement, log levels, and message generation}, aligning with practical goals. However, \textbf{accuracy} poorly handles class imbalance in log levels, and complementary metrics such as \textbf{F1-score} and confusion matrices are missing. \textbf{BLEU-4} is poorly suited for evaluating log messages, as even small semantic differences (e.g., ""error occurred"" vs. ""error handled"") can drastically alter the impact of the log statement. The dataset, derived from \textbf{Java projects using Log4j}, limits generalizability to other languages and frameworks. The assumption that every method requires a log statement is acknowledged by the authors as a \textbf{scope boundary} rather than an evaluation limitation.",3,3,-1,0,-1,1,1,Assumed correctness without validation,Missing baseline comparison,Limited metric selection,Manual validation of errors,Large-scale Java method dataset,nan
yu2023log,Log Parsing with Generalization Ability under New Log Types,2023,,,2,Log Parsing and Analysis,Log parsing,software_maintenance,generation,direct_solution,"16 Loghub datasets (e.g., HDFS, BGL, Android, Windows) (OS, IND)","Drain, Spell, LenMa, SHISO, Logram, Nulog","Grouping Accuracy, Edit Distance","Ablation (test-time training, variable imitation), Sensitivity analysis","Manually labeled subsets (2K logs/dataset), minor corrections applied ","Tested on diverse datasets, covering multiple domains and log types. Manual corrections were applied to labeled subsets, but comprehensive validation of the entire ground truth remains unclear. Cross-repository generalizability is not explicitly addressed.","Metrics align with practical goals, focusing on adaptability to new log types and efficiency at real-world log generation scales. However, deployment overheads and failure analysis remain unexplored.",3,4,1,0,1,1,1,Manually labeled with expert review,Limited baseline comparison,Task-appropriate metrics,Cross-validation and sensitivity analysis,Multiple diverse datasets,nan
ma2024knowlog,KnowLog: Knowledge Enhanced Pre-trained Language Model for Log Understanding,2024,,,2,Log Parsing and Analysis,Log parsing,software_maintenance,generation,direct_solution,"96K templates, Cisco, Huawei, H3C logs (OS, IND)","CNN, Bi-LSTM, BERT, RoBERTa, UniLog","Accuracy, Weighted F1, Precision@1, MRR",Low-resource analysis,"FPI: Expert-labeled (43 phenomena, multi-label, no conflict details); LDSM: Positive from documentation, negative sampled automatically ","Pre-training used 96,060 log templates from Cisco, Huawei, and H3C, filtered for duplicates, but no explicit statement clarifies if downstream test datasets contain \textbf{logs generated by repositories or vendors unseen during pre-training}, raising questions about cross-domain generalizability. Manual labeling details are inconsistent across tasks: while the \textbf{Fault Phenomenon Identification (FPI)} dataset was labeled by experts, no information is provided about the number of annotators, conflict resolution processes, or inter-annotator agreement. Similarly, in the \textbf{Log and Description Semantic Matching (LDSM)} task, positive pairs were derived directly from documentation, and negative pairs were artificially created, but the validation process is not described, leaving potential ambiguity in label quality.","The evaluation uses a variety of task-specific metrics. For \textbf{Log and Possible Cause Ranking (LPCR)}, metrics such as \textbf{Precision@1} and \textbf{Mean Reciprocal Rank (MRR)} are well-chosen due to the ranking nature of the task. Precision@1 focuses on ensuring the top-ranked cause is correct, aligning with practical use cases where developers prioritize the most likely root cause. MRR complements this by capturing the position of the first correct cause in the ranking, offering a more nuanced assessment. These metrics are more suitable than accuracy or recall for ranking problems. Across other tasks, F1-score effectively handles class imbalance in multi-label tasks like \textbf{Fault Phenomenon Identification (FPI)}, ensuring robust evaluation.",3,4,-1,1,1,0,1,Unclear annotation process for some tasks,Multiple baseline approaches,Task-appropriate metrics,Limited validation details,Large-scale multi-vendor dataset,nan
liu2024interpretable,Interpretable Online Log Analysis Using Large Language Models with Prompt Strategies,2024,,,2,Log Parsing and Analysis,Log parsing,software_maintenance,generation,direct_solution,"(HDFS, BGL, Zookeeper, Android) (OS, IND)","LogPPT, LogStamp, LogParse, LogSig, Spell, Drain, DeepLog, LogAnomaly, LogRobust","F1-score (session-level, template-level), Precision, Recall","Ablation (prompt strategies: Self-prompt, Chain-of-Thought, In-context), Human evaluation (6 experts, 200 logs)","Expert-annotated anomalies and parsing templates, potential pre-training data overlap","The study evaluates \textbf{LogPrompt} on nine datasets across two tasks: \textbf{log parsing} and \textbf{anomaly detection}. \textbf{F1-score} serves as the primary metric, effectively capturing task performance. \textbf{Ablation studies} assess prompt strategies (self-prompt, chain-of-thought (CoT), in-context prompt) incrementally, showing clear improvements in session-level and template-level F1-scores.  \textbf{Human validation is limited to six experts and 200 logs}, focusing only on correctly predicted samples, introducing potential bias. Additionally, the authors acknowledge potential \textbf{data leakage} from GPT-3.5's pre-training corpus.","The \textbf{F1-score}, as the primary metric, aligns with the task objectives addressing the imbalanced nature of anomaly detection and ensuring accurate variable extraction in log parsing. While OpenAI's API manages backend scalability, the study does not evaluate observable \textbf{deployment overheads} (e.g., latency, cost per log processed) or the impact of \textbf{prompt design constraints} (e.g., token limits, batching strategies) on scalability.",3,3,1,1,1,1,1,Expert annotations with conflict resolution,Multiple traditional and modern baselines,Comprehensive metrics suite,Ablation and human evaluation,Multiple datasets across domains,nan
xu2024unilog,UniLog: Automatic Logging via LLM and In-Context Learning,2024,,,2,Log Parsing and Analysis,logging,software_maintenance,generation,direct_solution,"12,012 code snippets from 1,465 GitHub repositories (OS)","LANCE, GPT-3 variants (Ada, Babbage, Curie), Codex (Fine-Tuned)","PA, LA, MA, CLA, CMA, BLEU-4","Ablation (warmup, example count, example order), Manual error analysis (500 samples)",Log statement correctness validated via annotated dataset and automated metrics ,"The evaluation systematically measures UniLog’s performance across three subtasks (logging position, verbosity level, log message generation) using automated metrics (\textbf{PA, LA, MA, CLA, CMA, BLEU-4}). Ablation studies investigate the impact of warmup strategies, prompt example counts, and example ordering. The dataset (12,012 snippets from 1,465 GitHub repositories) was preprocessed and standardized. However, class imbalance in verbosity levels is not addressed, and dataset representativeness across diverse domains remains unclear. While a qualitative manual error analysis of 500 log messages provides insights into common failure patterns (e.g., synonym replacements, order changes, meaningless information), it lacks structured developer feedback or reproducibility.","The evaluation metrics align with task goals, with \textbf{CLA} and \textbf{CMA} adding granularity to correctness analysis. However, BLEU-4 focuses on lexical similarity, limiting insights into semantic correctness and practical message quality. Deployment concerns (e.g., integration into CI/CD workflows, runtime scalability) are not addressed. Additionally, the manual error analysis, while insightful, does not include structured developer validation to ensure real-world relevance.",3,3,1,1,1,1,1,Validated correctness through annotations,Multiple LLM variants as baselines,Multiple task-specific metrics,Ablation studies and error analysis,Large dataset from diverse repositories,nan
biswas2020achieving,achieving reliable sentiment analysis in the software engineering domain using bert,2020,3.0,3.0,3,Sentiment Analysis,Sentiment analysis,software_maintenance,recommendation,existing_solution_evaluation,5500 SO sentences,RNN4SentiSE,"Precision, Recall, F-Measure (10-fold CV, Kappa: 0.88)","Manual, refined guidelines, cross-validation ","Manual, refined guidelines, cross-validation","Robust dataset construction with 5500 annotated Stack Overflow sentences validated by high inter-rater agreement (Cohen’s Kappa: 0.88). Macro-averaged F-measure ensures fairness across classes, addressing class imbalance effectively. Use of 10-fold cross-validation adds rigor, though reliance on a single annotator for new data introduces slight concern. Absence of real-world user validation slightly limits overall reliability.","Metrics focus on class-specific performance (e.g., precision, recall, F-measure), addressing key challenges like minority classes and domain-specific nuances. Results highlight suitability for practical SE tasks. However, generalizability is limited as the evaluation is confined to Stack Overflow posts, which may reduce the broader applicability of the findings to other types of SE data. The lack of explicit evaluation on actionable insights for practitioners slightly tempers relevance.",3,3,1,1,1,1,-1,High inter-rater agreement,Limited baseline comparison,Class-specific metrics,Cross-validation implemented,Limited to SO posts only,nan
zhang2020sentiment,sentiment analysis for software engineering: how far can pre-trained transformer models go?,2020,3.0,3.0,3,Sentiment Analysis,Sentiment analysis,software_maintenance,generation,existing_solution_evaluation,"API reviews, SO posts, app reviews, GitHub, Jira, CR (15K total)","CoreNLP, SentiStrength(-SE), SentiCR, Senti4SD","Precision, Recall, F1, Training Time (70/30 split, cross-dataset)","Manually labeled, SE-specific polarity ","Manually labeled, SE-specific polarity","Evaluation leverages five publicly available SE datasets curated from prior works with diverse class distributions. Metrics such as macro-F1, micro-F1, and AUC ensure fair performance across classes in imbalanced datasets. Comprehensive error analysis identifies issues like implicit sentiment polarity and subjective annotations, offering detailed insights into model limitations. Retaining original annotations ensures consistency with prior benchmarks.","Datasets represent real-world SE scenarios, addressing challenges like data scarcity and imbalanced distributions. Error analysis reveals practical obstacles in deployment, such as subjective annotations and implicit sentiment handling, directly aligning with real-world sentiment analysis needs. The evaluation’s focus on dataset quality and deployment challenges enhances its applicability for SE contexts.",4,4,1,1,1,1,1,Manual SE-specific annotations,Multiple sentiment tool baselines,Class-balanced metrics,Cross-dataset validation,Multiple SE domain datasets,nan
zhang2023revisiting,Revisiting Sentiment Analysis for Software Engineering in the Era of Large Language Models,2023,,,3,Sentiment Analysis,Sentiment analysis,software_maintenance,generation,existing_solution_evaluation / direct_solution,"SE datasets: Gerrit, GitHub, GooglePlay, Jira, StackOverflow (11K total)","sLLMs (BERT, RoBERTa), bLLMs (Llama2, Vicuna)","Precision, Recall, F1, AUC (80/10/10 split, Wilcoxon test)","Pre-labeled, multiple annotators, conflict resolution ","Pre-labeled, multiple annotators, conflict resolution","Evaluation leverages five publicly available SE datasets curated from prior works with diverse class distributions. Metrics such as macro-F1, micro-F1, and AUC ensure fair performance across classes in imbalanced datasets. Comprehensive error analysis identifies issues like implicit sentiment polarity and subjective annotations, offering detailed insights into model limitations. ","Datasets represent real-world SE scenarios, addressing challenges like data scarcity and imbalanced distributions. Error analysis reveals practical obstacles in deployment, such as subjective annotations and implicit sentiment handling, directly aligning with real-world sentiment analysis needs. The evaluation’s focus on dataset quality and deployment challenges enhances its applicability for SE contexts.",4,4,1,1,1,1,1,Multiple annotators with resolution,Multiple LLM type comparison,Comprehensive metric suite,Statistical significance testing,Multiple SE domain datasets,nan
wang2022your,Where is Your App Frustrating Users?,2022,3.0,3.0,3,User Feedback Processing,App Review clustering,software_maintenance,generation,direct_solution,"App reviews: 6 apps (3.4K labeled), 18 apps (318K unlabeled)","SAFE, Caspar, KEFE, BiLSTM-CRF, LDA, K-Means","Precision, Recall, F1 (extraction), ARI, NMI (10-fold CV, user survey)","Manual, iterative refinement (Kappa: 0.78–0.86) ","Manual annoation, iterative refinement (Kappa: 0.78–0.86)","The evaluation utilizes a well-constructed dataset of app reviews (3,426 reviews across six apps) with ground truth annotations validated by inter-rater agreement (Cohen's Kappa: 0.78–0.86), ensuring labeling accuracy.  An ablation study explores the role of review attributes (e.g., sentiment, app category) in extraction performance, providing insights into factors influencing model reliability. Comprehensive comparisons with state-of-the-art baselines add rigor to the assessment. The qualitative evaluation of the large-scale dataset (318,534 reviews) further demonstrates the method's practical usability and supports findings on scalability.","The study uses precision, recall, F1-score (for problematic feature extraction), and ARI/NMI (for clustering) to align the evaluation with real-world needs, ensuring metrics are suited to assessing both extraction and grouping tasks. Precision and recall are critical for accurately identifying problematic features, particularly given the imbalanced nature of extracted features where some are underrepresented. F1-score complements these metrics by balancing precision and recall. ARI and NMI validate clustering consistency and practical applicability, addressing the need for actionable developer insights from app reviews.",4,4,1,1,1,1,1,High inter-rater agreement,Multiple technique comparison,Multiple complementary metrics,Cross-validation and user study,Balanced labeled/unlabeled split,nan
he2022ptm4tag,ptm4tag: sharpening tag recommendation of stack overflow posts with pre-trained models,2022,2.0,3.0,3,User Feedback Processing,Tag recommendation,software_maintenance,classification,direct_solution,"Stack Overflow posts (10.38M train, 100K test)","Post2Vec, PTM4Tag (BERT, RoBERTa, CodeBERT, ALBERT, BERTOverflow)","Precision@k, Recall@k, F1@k (Fixed split, ablation, error analysis)","User-labeled tags, filtered rare/noisy tags ","User-labeled tags, filtered rare/noisy tags","The evaluation uses a large-scale dataset from Stack Overflow with user-generated tags as ground truth. Filtering processes exclude rare and noisy tags, ensuring a cleaner train/test dataset. However, no manual verification of the ground truth annotations introduces potential risks of label noise. The ablation study investigates the relative contribution of post components (Title, Description, Code) to tagging accuracy, shedding light on which data sources are most critical for the evaluated models. Rigorous comparisons between five PTM variants and a state-of-the-art CNN baseline reinforce the evaluation's robustness.","Evaluation metrics (Precision@k, Recall@k, F1-score@k) are well-suited to assessing multi-label tag prediction in real-world scenarios, particularly as they reflect how well the top-k recommended tags align with actual user needs. This aligns closely with practical applications of tag recommendation. Precision and recall address challenges posed by imbalanced tag distributions, while F1-score balances these for a more comprehensive assessment. The ablation study further informs real-world applicability by highlighting the importance of individual post components (e.g., Title) in achieving tagging accuracy.",4,4,1,1,1,1,1,User-labeled tags with filtering,Multiple PTM variant comparison,Comprehensive ranking metrics,Ablation and error analysis,Large-scale SO dataset,nan
motger2024t,T-FREX: A Transformer-based Feature Extraction Method from Mobile App Reviews,2024,,,3,User Feedback Processing,App review feature extraction,software_maintenance,classification,direct_solution,"App reviews: 468 apps (23.8K reviews, 32.4K annotated features)","SAFE (baseline), BERT, RoBERTa, XLNet","Precision, Recall, F1 (token \","feature-level), Precision@k, Recall@k (10-fold CV, lexical overlap analysis, ablation, human validation)","Crowdsourced (AlternativeTo), validated by human annotators (F1: 0.719) ","The evaluation utilizes a large, diverse dataset (468 apps, 23,816 reviews, 32,443 annotated features) sourced from a crowdsourced software recommendation platform (\textit{AlternativeTo}). The dataset is enriched with app metadata and refined annotations. Evaluation spans both \textbf{in-domain} (fine-tuned on the same app category) and \textbf{out-of-domain} (evaluated on unseen categories) scenarios, ensuring robust assessment across contexts. Metrics are calculated at both \textbf{token-level} and \textbf{feature-level} granularity, with an additional \textbf{human validation phase} for newly predicted features via crowdsourced annotation platforms. High inter-rater agreement (F1: 0.719) underscores annotation reliability. Ablation studies, lexical overlap analysis, and fine-grained category-level insights further enhance reliability.","The study employs \textbf{Precision, Recall, F1-score} (at both token and feature levels) alongside \textbf{Precision@k} and \textbf{Recall@k}, reflecting real-world use cases. The evaluation framework emphasizes identifying features that are actionable and practically relevant for app developers. Human validation ensures that newly discovered features align with real-world user expectations. However, the evaluation does not explicitly assess whether these features provide actionable insights for app developers, slightly limiting practical relevance. The dual in-domain and out-of-domain configurations demonstrate applicability across both stable and novel app domains, while fine-tuning capabilities address domain-specific nuances effectively.",4,4,1,1,1,1,1,Crowdsourced with expert validation,Multiple transformer baselines,Multiple granularity metrics,Cross-validation and human validation,Large diverse app review dataset,nan
hey2020norbert,NoRBERT: Transfer Learning for Requirements Classification,2020,3.0,3.0,4,Requirements Classification ,Requirements classification,software_requirements,generation,direct_solution,"PROMISE NFR, relabeled version","Feature-based classifiers, Tree, SVM, Naïve Bayes, CNN-based approach","Precision, Recall, F1, Weighted F1","Stratified 10-fold CV, loPo ","Human-annotated labels (Functional and 11 NFR subclasses in the original dataset; Functional (F), Quality (Q), and Both (F+Q) in the relabeled dataset).","Robust evaluation with loPo and p-fold methods ensures generalization to unseen projects. Effective sampling addresses class imbalance. However, reliance on the outdated PROMISE NFR dataset limits adaptability to modern requirements data.","Validation mimics real-world scenarios, where models generalize across projects. Binary and multi-class classification are evaluated using metrics such as F1-Score and Weighted F1 for thorough quantitative assessment. Lack of qualitative evaluation, such as user feedback or case studies, limits insights into broader applicability.",3,3,1,1,1,1,-1,Human-annotated labels,Multiple classifier comparison,Class-balanced metrics,Cross-validation variants,Limited to PROMISE dataset,nan
luo2022prcbert,PRCBERT: Prompt Learning for Requirement Classification using BERT-based Pretrained Language Models,2022,3.0,3.0,4,Requirements Classification ,Requirements classification,software_requirements,generation,direct_solution + New Dataset (NFR-SO),"PROMISE NFR, NFR-Review, NFR-SO","NoRBERT hey2020norbert, BERT-MLM, Trans\_PRCBERT","Precision, Recall, F1, Weighted F1, T-test","10-fold CV, repeated trials ","Human-annotated labels from PROMISE (Functional and 11 NFR subclasses), NFR-Review (5 NFR categories), and NFR-SO (7 NFR categories).","Diverse datasets and robust validation methods (cross-validation, T-tests) enhance the evaluation's robustness. The inclusion of newer datasets, such as NFR-SO, addresses the limitations of older datasets like PROMISE, which may not reflect contemporary requirements data and potential concept drift.","By incorporating NFR-SO, the evaluation overcomes the limitations of outdated datasets like PROMISE, addressing the evolution of requirements data over time. Evaluation tasks include binary, multi-class, and zero-shot classification, reflecting practical real-world scenarios. However, reliance on StackOverflow data limits direct applicability to industrial requirements.",3,3,1,1,1,1,1,Multi-category human annotations,Multiple BERT variant comparison,Multiple classification metrics,Cross-validation and statistics,Multiple requirements datasets,nan
el2023ai,"Which AI Technique Is Better to Classify Requirements? An Experiment with SVM, LSTM, and ChatGPT",2023,,,4,Requirements Classification ,Requirements classification,software_requirements,classification,existing_solution_evaluation,"PROMISE NFR, Dronology, ReqView, Leeds Library, WASP","SVM, LSTM, GPT-3.5, GPT-4 (zero/few-shot)",F$_\beta$-Score,Cross-dataset testing ,"Human-labeled requirements datasets (PROMISE, Dronology, ReqView, Leeds Library, WASP).","Metrics such as F$_\beta$-Score effectively address class imbalance, and cross-dataset testing provides insights into the evaluation’s generalization capabilities. The inclusion of diverse datasets enhances reliability by testing the evaluation approach across various real-world scenarios. Additionally, the selection of diverse baselines offers insights into the performance of different classifier categories, strengthening the evaluation's comprehensiveness. However, small dataset sizes limit statistical robustness.","The evaluation provides valuable insights into performance under different usage scenarios by comparing zero-shot and few-shot prompting approaches. Additionally, the use of diverse datasets strengthens the evaluation's ability to reflect real-world challenges in requirements classification. However, the small datasets constrain the breadth of generalization to broader industrial contexts.",3,4,1,1,1,1,1,Human-labeled requirements,Multiple paradigm comparison,Class-balanced metrics,Cross-dataset testing,Multiple domain datasets,nan
ezzini2022automated,Automated Handling of Anaphoric Ambiguity in Requirements: A Multi-Solution Study,2022,3.0,3.0,4,Ambiguity Detection,Anaphoric Ambiguity Detection,software_requirements,classification,existing_solution_evaluation,"DAMIR, ReqEval, CoNLL2011",None (standalone eval. with different config),"F2-Score, Success Rate","10-fold CV, external datasets ",Human expert annotations (ambiguity detection and antecedent labeling). DAMIR annotations were driven by inter-annotator agreement.,"Robust evaluation using diverse datasets, expert annotations, and metrics addressing class imbalance. Cross-validation and separate test splits enhance confidence in the evaluation's ability to generalize. However, reliance on manual annotation may limit scalability.","Prioritizing recall ensures ambiguous cases are flagged for review, which is crucial because missing these ambiguities in early project stages can lead to significant costs in later phases. Partial matching for resolution aligns with practical needs in real-world requirements engineering. Evaluation across a large set of industrial requirements provides strong insights into generalizability, though the inclusion of open-source datasets would have further strengthened the evaluation's ability to assess broader applicability. The absence of qualitative studies limits a deeper understanding of the evaluation's outcomes.",4,4,1,1,1,1,1,Expert annotations with agreement,"The study compares six alternative solutions across distinct paradigms: two SpanBERT-based models (fine-tuned on CoNLL2011 and RE-specific DAMIR datasets), three ML-based models (using linguistic features, embeddings from BERT/SBERT, and an ensemble of both), and one NLP coreference resolver-based solution (leveraging SpaCy and CoreNLP). This multi-paradigm evaluation extends beyond simple configuration comparisons, offering a comprehensive analysis across diverse techniques.",Recall-focused metrics,Cross-validation approach,Diverse dataset sources,"SpanBERT-based models, ML-based models (linguistic features), ML-based models (embeddings from BERT/SBERT), ML-based ensemble model, NLP coreference resolver (SpaCy), NLP coreference resolver (CoreNLP)"
moharil2022identification,identification of intra-domain ambiguity using transformer-based machine learning,2022,3.0,3.0,4,Ambiguity Detection,Intra-domain Ambiguity detection,software_requirements,classification,direct_solution,"Domain-Specific Corpus, Multi-Domain Corpus",None (evaluated standalone),None (manual validation),Qualitative analysis ,No quantitative ground truth; relies on manual validation without inter-annotator agreement metrics or standardized annotation process.,"Lack of standardized validation metrics (e.g., cluster purity) and reliance on manual review without inter-annotator agreement affect reproducibility. Absence of baselines further reduces reliability.","Using a Wikipedia corpus reduces applicability to requirements engineering tasks, as it differs significantly from requirements documents in structure and specificity.",1,2,-1,-1,-1,-1,-1,No quantitative ground truth,No baseline comparison,No quantitative metrics,Qualitative analysis only,Non-requirements corpus,nan
moharil2023tabasco,TABASCO: A transformer based contextualization toolkit,2023,3.0,3.0,4,Ambiguity Detection,Anaphoric Ambiguity Detection,software_requirements,classification,direct_solution,"CS Domain Corpus, PURE dataset",None (evaluated standalone),None (manual inspection),Qualitative analysis ,Not applicable; evaluation is based on manual qualitative analysis without labeled data.,Manual qualitative analysis without quantitative metrics introduces subjectivity. Lack of inter-annotator agreement and baselines limits comparative context.,"The use of the PURE dataset enhances practical relevance. However, reliance on Wikipedia for the CS domain corpus may reduce applicability to real-world requirements documents, which often differ in structure and style. The lack of quantitative metrics and practitioner involvement limits the assessment's applicability to practical settings in requirements engineering.",2,3,-1,-1,-1,-1,-1,No labeled data validation,No baseline comparison,No quantitative metrics,Qualitative analysis only,Non-requirements corpus,nan
wang2020deep,A Deep Context-wise Method for Coreference Detection in Natural Language Requirements(detecting coreferent entities in natural language requirements),2022,3.0,3.0,4,Ambiguity Detection,Coreference detection,software_requirements,classification,direct_solution,Industry partner dataset (21 projects),None (evaluated standalone),"Precision, Recall, F1",10-fold CV ,Human-annotated labels reviewed by requirements engineers and project management teams (coreferent vs. non-coreferent pairs).,"Results are derived from an undersampled balanced dataset, obscuring the model's performance under natural class distributions. This undermines both reliability and real-world relevance.",Evaluation lacks alignment with real-world conditions due to reliance on a synthetic balanced dataset.,2,2,1,-1,1,1,-1,Expert-reviewed annotations,No baseline comparison,Standard classification metrics,Cross-validation approach,Evaluation on an undersampled dataset; lacks transparency regarding original class distribution,nan
ronanki2022chatgpt,ChatGPT as a tool for User Story Quality Evaluation: Trustworthy Out of the Box?,2023,3.0,3.0,4,Automated Requirements Evaluation,Requirement analysis and evaluation,software_requirements,classification,existing_solution_evaluation,Open-source user stories,AQUSA,"Agreement Rate, Precision, Recall, F1, Specificity",Three trials with one-shot prompting ,Double-blinded human evaluation,"Compares ChatGPT's outputs against human assessments (ground truth) and the AQUSA tool (baseline). GPT was prompted three times, but missing configuration details (e.g., temperature) hinder reproducibility. Small sample size (11 requirements), lack of inter-annotator agreement scores, and restriction to open-source projects limit robustness.","Provides practical insights into user story quality by assessing seven criteria. Specificity as a metric addresses trustworthiness by revealing whether ChatGPT tends to over-accept user stories as meeting quality criteria, mitigating risks of false positives. Reliance on open-source requirements limits generalizability, and the small sample size reduces applicability to diverse contexts. Absence of qualitative insights on real-world workflow impacts leaves room for improvement.",2,4,1,1,1,1,-1,Double-blind human evaluation,Single tool comparison,Multiple effectiveness metrics,Multiple trial evaluation,Small requirements sample,nan
lin2021traceability,traceability transformed: generating more accurate links with pre-trained bert models,2021,3.0,3.0,4,Requirements Traceability and Completeness,Requirement traceability,software_requirements,recommendation,direct_solution,"Flask, Pgcli, Keras","VSM, LDA, LSI, TraceNN","F1, F2, MAP@3, MRR, Precision@K","10-fold CV, ONS, DRNS ",Issue-commit links mined from commit messages using developer-embedded issue tags. Potential incompleteness acknowledged.,"Moderate to large-scale test sets from diverse application types (e.g., databases, web frameworks, neural networks) and comparisons with established baselines enhance validity. However, the evaluation is limited to open-source projects in Python, and potential incompleteness of mined links from commit messages may introduce bias.","Metrics like MAP@3 and F2 address practical needs—recall is critical in early stages of traceability to ensure all relevant links are identified. The reliance on open-source Python projects limits generalizability to other languages and industrial contexts, reducing the broader applicability of the evaluation.",3,4,-1,1,1,1,-1,Potential link incompleteness,Multiple technique comparison,Multiple ranking metrics,Cross-validation with variants,Limited to Python projects,nan
zhu2022enhancing,Enhancing Traceability Link Recovery with Unlabeled Data,2022,3.0,3.0,4,Requirements Traceability and Completeness,Traceability recovery,software_maintenance,recommendation,direct_solution,"Flask, Pgcli, Keras",TRACEFUN (variants),"F1, F2, MAP",5-fold CV ,Issue-commit links inherited from Lin et al. (2021); potential incompleteness acknowledged.,"Relies on the same mined datasets as \cite{lin2021traceability}, inheriting limitations such as potential bias from incomplete traceability links.","Metrics like F1 and F2 provide meaningful insights into ranking performance. However, using MAP without a cutoff reduces the emphasis on top-ranked results, limiting its alignment with practical traceability needs. The reliance on open-source datasets restricts insights into industrial contexts and other programming languages, similar to the limitations identified in \cite{lin2021traceability}.",3,3,-1,1,1,1,-1,Inherited potential incompleteness,Multiple variant comparison,Multiple ranking metrics,Cross-validation approach,Limited to specific projects,nan
poudel2023leveraging,Leveraging Transformer-based Language Models to Automate Requirements Satisfaction Assessment,2023,,,4,Requirements Traceability and Completeness,Requirement analysis and evaluation,software_requirements,classification,direct_solution,Five hierarchical datasets,Legacy IR-based chunking,"F2, MAP","5-fold CV, repeated trials ",Manually curated Requirements Traceability Matrices (RTMs).,"Evaluation uses five datasets from different application domains. Repeating experiments across five unique train-validation-test splits with varied seeds ensures consistent and robust results. However, the datasets are limited in size.","Metrics such as F2 prioritize recall, essential for identifying inaccuracies in requirement satisfaction in early project phases, particularly in safety-critical systems. MAP evaluates ranking quality, aligning well with real-world traceability and satisfaction assessment needs. ",3,4,1,1,1,1,-1,Manual RTM curation,Single baseline comparison,Recall-focused metrics,Cross-validation approach,Limited dataset size,nan
luitel2024improving,Improving Requirements Completeness: Automated Assistance through Large Language Models,2023,3.0,3.0,4,Requirements Traceability and Completeness,Code Example Recommendations,software_development,recommendation,direct_solution,PURE dataset (40 specs),"Three baselines: Most common words, TF-IDF, WordNet synonyms","Accuracy, Coverage, Precision, Recall","5-fold, Wilcoxon, A\textsuperscript{12, human eval ",Simulated incompleteness by withholding portions of requirements and using them as reference for missing terminology.,Diverse requirements datasets and cross-validation enhance reliability. Inter-annotator agreement supports evaluation consistency.,"Metrics like Accuracy and Coverage ensure completeness and precision, aligning with practical needs. Simulating incompleteness, however, may not fully represent real-world scenarios. Broader datasets and practitioner involvement could improve applicability.",3,3,1,1,1,1,-1,Simulated incompleteness validation,Multiple approach comparison,Multiple effectiveness metrics,Cross-validation and human eval,Limited specifications sample,nan
yang2022aspect,aspect-based api review classification: how far can pre-trained transformer model go?,2022,3.0,3.0,5,Program Specifications,Code review and software/code classification,software_maintenance,classification,existing_solution_evaluation,"4,522 Stack Overflow API sentences","Opiner, BERT-family, CostSensBERT","Weighted P/R/F1, MCC, AUC","10-fold CV, manual analysis",Manual labels (substantial agreement) ,"The evaluation uses a well-annotated dataset with substantial inter-annotator agreement, multiple metrics capturing different performance aspects, and a standard 10-fold cross-validation. Comparing various PTMs and Opiner across conditions (original vs. undersampled training data) helps confirm consistent performance patterns rather than relying on a single approach.","Metrics like Weighted F1, MCC, and Weighted AUC address class imbalance and reflect practical conditions often encountered in real-world requirements analysis tasks. The manual error analysis provides qualitative insights into model behavior, aligning the evaluation with real-world interpretability and utility.",4,4,1,1,1,1,1,High inter-annotator agreement,Multiple model comparison,Class-balanced metrics,Cross-validation and analysis,Well-annotated API dataset,nan
yang2023apidocbooster,APIDocBooster: An Extract-Then-Abstract Framework Leveraging Large Language Models for Augmenting API Documentation,2023,,,5,API Documentation and Augmentation,API documentation augment,software_development,generation,direct_solution,"APISumBench (4,344 sentences, 48 summaries)","SISE, DeepTip, LexRank, TechSumBot++","P/R/F1 for classification, ROUGE for summarization","Cross-/Within-API splits, Human review (Likert ratings)","Labeled sentences, Extractive summaries ","The evaluation employs a robust annotation process with substantial inter-annotator agreement and utilizes cross-API and within-API splits to assess model generalization across different APIs. Additionally, the combination of automatic metrics and human evaluations ensures robust and reproducible results. However, the exact number of samples manually reviewed in the human evaluation is not specified.","The evaluation strategy prioritizes Weighted Precision to address class imbalance, ensuring accurate identification of relevant API documentation sections, which is crucial for maintaining the quality and reliability of the augmented documentation. An acceptable level of Recall is maintained to ensure comprehensive coverage of relevant information. The high-quality ground truth, supported by substantial inter-annotator agreement, ensures that the evaluation accurately reflects necessary documentation attributes. Human evaluations focus on key attributes that align directly with the needs of the practitioner and ensure applicability to real-world documentation scenarios.",4,4,1,1,1,1,1,Strong inter-annotator agreement,Multiple technique comparison,Task-specific metrics,Cross-API validation,Comprehensive API dataset,nan
mandal2023large,large language models based automatic synthesis of software specifications,2023,2.0,3.0,5,Program Specifications,Software specification synthesis,software_design,classification,direct_solution,"SpecSyn (300 real specs, 3000 synthetic)",PracExtractor,"Precision, Recall, F1 Score",By software types and categories,Manually curated specs from diverse sources ,"The evaluation strategy is undermined by a lack of transparency in the creation and distribution of the non-specification (non-spec) class within the test set. The test set is small (250 samples) and predominantly synthetic, with a limited number of real specifications (likely 25), leading to a low confidence in reported metrics. Additionally, the ground truth is narrowly derived from API documentation rather than diverse real-world specification contexts. Missing details on annotation practices further detract from reliability.","While the evaluation focuses on Precision and Recall, which are critical for specification synthesis tasks, the reliance on synthetic data skews the applicability to real-world contexts. The dataset’s limited diversity, lack of integration with realistic artifacts such as requirements, and small size reduce its ability to generalize to real-world scenarios. These issues compromise the ability to assess how well the solution would perform in varied and complex environments.",2,3,-1,-1,-1,-1,-1,Limited real specifications,Single extractor comparison,Basic classification metrics,Limited validation approach,Predominantly synthetic data,nan
xie2023impact,Impact of Large Language Models on Generating Software Specifications,2023,,,5,Program Specifications,Specification generation,software_requirements,generation,existing_solution_evaluation,"Jdoctor (854), DocTer (2,876)","Jdoctor, DocTer","Accuracy, Precision, Recall, F1 Score","Leave-One-Out CV, Failure Analysis","Annotated datasets, corrected for semantic equivalence ","The evaluation demonstrates a robust methodology with extensive experiments across 15 state-of-the-art LLMs and two distinct datasets, supported by a manual review process to account for semantic equivalence in outputs. While the lack of transparency in the quantity and outcomes of manual reviews is a limitation, the inclusion of edge case analyses and the use of cross-validation with Few-Shot Learning (FSL) enhance reliability. However, more details on failure analysis and example selection would strengthen confidence.","The evaluation employs metrics directly tied to the task, such as Accuracy, Precision, Recall, and F1, and analyzes performance across multiple dimensions, including correctness and generalizability. The use of semantic retrieval and diverse prompt sizes reflects a focus on real-world applicability.",4,4,1,1,1,1,1,Semantic equivalence validation,Multiple tool comparison,Multiple effectiveness metrics,Cross-validation approach,Multiple validated datasets,nan
ma2024specgen,SpecGen: Automated Generation of Formal Program Specifications via Large Language Models,2024,,,5,API Documentation and Augmentation,Specification generation,software_requirements,generation,direct_solution,SV-COMP (265 Java); SpecGenBench (120); Defects4J (50 files),"Houdini, Daikon, AutoSpec, LLM","Verifier Calls, Passes, Success Rate, User Rating","Comparative study, User survey",Expert specs; Verification-guided benchmarks ,"Evaluation uses diverse datasets, including SV-COMP and SpecGenBench, alongside manually written ground truth specifications, enhancing rigor. The reliance on OpenJML, with known limitations, and lack of detail on expert annotation guidelines slightly impact reliability."," Metrics such as \textbf{Number of Verifier Calls}, \textbf{Success Probability}, and \textbf{User Ratings} directly assess efficiency, robustness, and usability. The datasets’ diversity supports evaluation across a wide range of scenarios, ensuring strong alignment with real-world applicability.",4,5,1,1,1,1,1,Expert specifications,Multiple tool comparison,Multiple effectiveness metrics,Comparative and user study,Diverse verification datasets,nan
endres2023formalizing,Formalizing Natural Language Intent into Program Specifications via Large Language Models,2023,,,5,Program Specifications,Specification formalization,software_requirements,generation,direct_solution,EvalPlus (164 Python problems); Defects4J (525 Java bugs),"TOGA, Daikon, GPT-family, StarChat","Accept@k, Bug-Completeness, Qualitative Analysis","Comparative study, Manual analysis","Expert-verified annotations, Test suite with code mutants ","Expert-verified annotations establish ground truth for correctness, supporting evaluation rigor. Benchmarks (EvalPlus and Defects4J) include diverse real-world examples, ensuring coverage across multiple programming languages and scenarios. However, potential overlap with LLM training data and limited diversity of certain ground-truth contexts reduce reliability.","Task-specific quantitative metrics, such as Accept@K and bug-completeness, effectively assess correctness and discriminative power of postconditions, directly addressing the task’s goal of detecting real-world bugs. The qualitative analysis categorizes generated postconditions into atomic and conjoined components, revealing patterns in correctness and bug-catching ability, making the findings applicable to practical debugging and runtime validation scenarios.",4,4,1,1,1,1,1,Expert-verified annotations,Multiple LLM comparison,Task-specific metrics,Comparative analysis,Multiple program datasets,nan
zhang2024experimenting,Experimenting a New Programming Practice with LLMs,2024,,,7,Requirements Elicitation and System Design,"Use cases generation, System desgin, Code generation","software_requirements, software_design, software_development",generation,direct_solution,"CAASD (72 software tasks, avg. 240 LoC/task)","ChatDev, MetaGPT","Pass Rate (\%), Token Cost","Manual testing, automatic testing",Reference use cases for functional requirements ,"The evaluation demonstrates robustness by introducing a novel benchmark (CAASD) with 72 tasks and human-in-the-loop validation for correctness. However, limited transparency regarding the origins of the 72 tasks and the process of creating the reference use cases raises concerns about task diversity and the ground-truth completeness. Additionally, the lack of details on tester backgrounds, user proficiency, and testing consistency further affects reliability. Despite these limitations, comparisons with baselines (ChatDev and MetaGPT) and the consistent application of quantitative measures enhance the robustness of the findings.","The evaluation employs task-specific and real-world metrics, such as pass rate for use case validation and token consumption for efficiency, effectively assessing the framework’s ability to complete practical software development tasks. Human engagement is quantitatively analyzed (e.g., number of revisions, pass rate improvements), showcasing its real-world applicability. However, aspects such as time improvement and perceived helpfulness of the tool were not assessed.",3,4,0,1,1,1,-1,Limited validation details,Multiple LLM tool comparison,Task-specific metrics,Manual and automatic testing,Limited task diversity,nan
kolthoff2023data,data-driven prototyping via natural-language-based gui retrieval,2023,3.0,3.0,7,Prototyping and GUI Retrieval,GUI retrieval,software_design,classification,direct_solution + New Evaluation Method,"Rico GUI dataset (57,764 GUIs), Crowdsourced Gold Standard (100 NL queries, top-20 GUIs/query)","TF-IDF, BM25, nBOW, PRF-KLD, Sentence-BERT","Precision@k, NDCG@k, MRR, HITS@k, Avg. Precision","Crowdsourced query creation, relevance annotations (AMT), user study (19 participants, real-world tasks)","GUI relevance annotations (3 independent workers/query, majority vote, filtered for quality) ","The evaluation employs rigorous methods, including the creation of a high-quality gold standard dataset with natural language queries generated by workers and relevance annotations by three independent annotators, ensuring reliability through inter-annotator agreement. A user study with 19 participants further validates the findings in real-world conditions, targeting novice developers as a key demographic. Experimental setups included measures to counterbalance order effects by randomizing task-tool assignments, ensuring fairness. However, some learning effects between tools might persist, and the relatively small participant pool slightly limits generalizability. Additionally, a user study effectively complements retrieval performance metrics (e.g., NDCG@k and HITS@k) with user effort metrics, such as the number of GUI components added, diversity, and correctness, highlighting both retrieval quality and practical prototyping efficiency.","The evaluation effectively assesses real-world applicability through metrics like NDCG@k and HITS@k for ranking quality, as well as user-perceived usefulness. The user study compares the proposed tool against a competitive baseline (Mockplus) under simulated real-world conditions, emphasizing the solution’s efficiency, usability, and perceived value. Productivity-focused metrics (e.g., number of GUI components added, diversity, and presence of irrelevant components) provide actionable insights into the tool’s effectiveness for prototyping tasks. However, concerns about dataset diversity and the lack of qualitative discussions on the correctness or completeness of final prototypes slightly temper its relevance.",4,4,1,1,1,1,1,High-quality crowdsourced annotations,Multiple retrieval baselines,Multiple ranking metrics,Comprehensive user study,Large-scale GUI dataset,nan
